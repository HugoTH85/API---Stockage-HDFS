services:
  hadoop-master:
    image: madjidtaoualit/hadoop-cluster:latest
    container_name: hadoop-master
    hostname: hadoop-master
    networks:
      - hadoop
    ports:
      - "9870:9870"
      - "8088:8088"
      - "7077:7077"
      - "16010:16010"
    volumes:
      - ../student.csv:/root/student.csv
    command: bash -c "service ssh start && /root/start-hadoop.sh && hdfs dfs -mkdir -p /user/root && hdfs dfs -put student.csv && tail -f /dev/null"

  hadoop-worker1:
    image: madjidtaoualit/hadoop-cluster:latest
    container_name: hadoop-worker1
    hostname: hadoop-worker1
    networks:
      - hadoop
    ports:
      - "8040:8042"
    command: bash -c "service ssh start && tail -f /dev/null"

  hadoop-worker2:
    image: madjidtaoualit/hadoop-cluster:latest
    container_name: hadoop-worker2
    hostname: hadoop-worker2
    networks:
      - hadoop
    ports:
      - "8041:8042"
    command: bash -c "service ssh start && tail -f /dev/null"
    
  spark-api:
    build:
      context: ./spark-api
    container_name: spark-api
    networks:
     -  hadoop
    ports:
      - "5000:5000"
    environment:
      - HDFS_HOST=hadoop-master
      - HDFS_PORT=9870
    command: ["python", "app.py"]

networks:
  hadoop:
    driver: bridge
